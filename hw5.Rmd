---
title: "hw5 jw4007"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### q2
#### import data
```{r}
homicide_csv = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicide = read.csv(homicide_csv)
```

#### create the "city_state" variable and summarize within cities to obtain the total number of homicides and the number of unsolved homicides
```{r}
homicide = homicide %>%
  mutate(city_state = str_c(city, state, sep = ","))

describe = homicide %>%
  group_by(city) %>%
  summarize(total_cases = n(),
            total_unsolved = sum(disposition == c("Closed without arrest", "Open/No arrest")))

knitr::kable(describe)
```
After creating the "city_state" variable, there are `r nrow(homicide)` observations and `r ncol(homicide)` variables in the dataset. The variables are `r names(homicide)`. 
The above table summarizes the total number of homicide cases and the number of unsolved homicides for the 50 large U.S. cities.

```{r}
describe_baltimore =
  describe %>%
  filter(city == "Baltimore") 

baltimore_test = prop.test(
  x = describe_baltimore[["total_unsolved"]],
  n = describe_baltimore[["total_cases"]]) %>%
  broom::tidy() %>%
  select(estimate, conf.low, conf.high)

knitr::kable(baltimore_test)
```
The above table describes the estimate the proportion of homicides that are unsolved and its confidence intervals from the city of Baltimore, MD.

```{r}
city_prop_test_fun = function(city_df) {
  
  city_summary = city_df %>%
    summarize(total_cases = n(),
              total_unsolved = sum(disposition == c("Closed without arrest", "Open/No arrest")))
  
  test = 
    prop.test(
    x = pull(city_summary, total_unsolved),
    n = pull(city_summary, total_cases)) %>%
    broom::tidy() %>%
    select(estimate, conf.low, conf.high)
  
  test
}

city_results =
  homicide %>%
  nest(data = uid:disposition) %>%
  mutate(results = map(data, city_prop_test_fun)) %>%
  select(city_state, results) %>%
  unnest(results)

```


```{r}
city_results %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = estimate, y = city_state)) + geom_point() + geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  labs(
    title = "Proportion of Unsolved Homicides"
    , x = "Proportion"
  )
``` 
### q3

Set the following design elements:

Fix n=30
Fix σ=5
Set μ=0. Generate 5000 datasets from the model
x∼Normal[μ,σ]

First, create a list of 5000 data that follow x∼Normal[0,5] with the sample size equals to 30.
```{r}
norm_list = vector("list", length = 50)
for (i in 1:5000) {
  norm_list[[i]] = rnorm(n = 30, sd = 5, mean = 0)
}
```

Second, create a function that apply t test and reports the estimate values and p.values from the test.
```{r}
t_test = function(x) {
  
  results = t.test(x) %>% 
  broom::tidy() %>%
  select(estimate, p.value)
  
  results

}
```

Third, construct a data frame combining the results of each iteration.
```{r}
test_results = tibble(
  mu = 0,
  n = 30,
  sd = 5,
  t_test_results = map_df(norm_list, t_test)
) %>%
  unnest(t_test_results) 
```

Repeat the above for μ={1,2,3,4,5,6}

```{r}
t_test_results = expand.grid(
  iteration = 1:5000,
  n = 30,
  sd = 5,
  mu = 0:6
  ) %>%
  mutate(list_n = map(.x = mu, ~rnorm(n = 30, sd = 5, mean = .x))) %>%
  mutate(ttest_results = map_df(list_n, t_test)) %>%
  unnest(ttest_results) %>%
  select(-list_n) %>%
  mutate(estimate = round(estimate, digits = 4),
         p.value = round(p.value, digits = 4),
         reject_null = if_else(p.value < 0.05, 1, 0))
```
Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.

```{r}
t_test_results %>%
  group_by(mu) %>%
  summarize(reject_pr = sum(reject_null == "1")/n()) %>%
  ggplot(aes(y = reject_pr, x = mu)) + geom_point() + 
  labs( x = "true mean",
       y = "power of the test")
```
From the plot above, we can see that the bigger the effect size, the larger the power.


```{r}
df_1 = 
  t_test_results %>% 
  mutate(mu = as.factor(mu))

df_2 = 
  t_test_results %>% 
  filter(reject_null == "1") %>%
  mutate(mu = as.factor(mu))

ggplot(data = df_1, aes(y = mean(estimate), x = mu)) + geom_point() + geom_point(data = df_2, color = "red") + labs(x = "mu",
       y = "average estimate of μ hat")


```
The above graph shows that the sample average of μ̂  across tests for which the null is rejected (red) does not equal to the true value of μ. It is because that the average estimate of μ̂ should be further from the null value for those observations to have the power to reject the null hypothesis.


```{r}
```



